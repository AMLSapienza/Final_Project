{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Test code written by Viresh Ranjan\n",
    "\n",
    "Last modified by: Minh Hoai Nguyen (minhhoai@cs.stonybrook.edu)\n",
    "Date: 2021/04/19\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "from model import  Resnet50FPN,Wide_Resnet50_2,VGG16FPN,CountRegressor,weights_normal_init\n",
    "from utils_ltce import MAPS, Scales, Transform, extract_features\n",
    "from utils_ltce import MincountLoss, PerturbationLoss\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torchvision.ops.boxes as bops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/Users/alessandroquattrociocchi/Git/AML/Final_Project/data/'\n",
    "data_path = '/Users/alessandroquattrociocchi/Documents/Courses /2/2.1/AML/homeworks/FinalProject/Final_Project/data/'\n",
    "output_dir = \"./logsSave\"\n",
    "test_split = \"test\" #choices=[\"train\", \"test\", \"val\"]\n",
    "gpu_id = 0 \n",
    "learning_rate = 1e-5\n",
    "data_path = data_path\n",
    "anno_file = data_path + 'annotation_FSC147_384.json'\n",
    "data_split_file = data_path + 'Train_Test_Val_FSC_147.json'\n",
    "im_dir = data_path + 'images_384_VarV2'\n",
    "gt_dir = data_path + 'gt_density_map_adaptive_384_VarV2'\n",
    "pre_trained_backbone = 'resnet' #choices=[resnet,wide_resnet,vgg16]\n",
    "model_path = data_path + \"/pretrainedModels/FamNet_Save1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/alessandroquattrociocchi/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-12-8 torch 1.10.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Model Summary: 476 layers, 76726332 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5l6', pretrained=True)\n",
    "\n",
    "def count_class(results):\n",
    "\n",
    "    dict_ = defaultdict(int)\n",
    "    for i in results:\n",
    "        dict_[i['name']] += 1\n",
    "    num = max(dict_.values())\n",
    "\n",
    "    return num\n",
    "\n",
    "\n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "def intersects(box1, box2):\n",
    "\n",
    "    box1 = torch.tensor([box1], dtype=torch.float)\n",
    "    box2 = torch.tensor([box2], dtype=torch.float)\n",
    "    iou = bops.box_iou(box1, box2).numpy()[0][0]\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLO_boxes(results_yolo, annotations, threshold=4):\n",
    "    \"\"\"\n",
    "    E.g. annotations = annotations['6.jpg']['box_examples_coordinates']\n",
    "    This function takes as input the results of YOLO and returns a set of boxes which will then be used for the\n",
    "    feature extraction part. In order to do this we will review a few things:\n",
    "    1. Yolo needs to detect at least 3 or more objects. If less than this number of objects has been detected we remove this observation\n",
    "    2. We keep only information regarding the most common object (which is typically the object we are trying to identify)\n",
    "    3. We remove the boxes that are overlapping (we don't want to provide FamNet with two similar boxes)\n",
    "    4. Keep only the boxes that are over a certain threshold level\n",
    "        a. if threshold is int take top threshold\n",
    "        b. if threshold is float take values above threshold\n",
    "    5. We provide as output the boxes that fulfill all of the previous requirements\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove all classes that don't belong to the most common class\n",
    "    list_ = []\n",
    "    for i in results_yolo:\n",
    "        list_.append(i['name'])\n",
    "    \n",
    "    if len(list_) == 0:\n",
    "        return None\n",
    "\n",
    "    name = most_frequent(list_)\n",
    "    final_dict = [i for i in results_yolo if i['name'] == name]\n",
    "\n",
    "    # More than 3 objects detected by YOLO\n",
    "    if len(final_dict) < 3:\n",
    "        return None\n",
    "\n",
    "    # Check for overlaps \n",
    "    non_overlap = list(range(1, len(results_yolo)+1))\n",
    "    overlap = []\n",
    "    for example_box in annotations:\n",
    "        list_original = [example_box[0][0], example_box[0][1], \n",
    "                        example_box[2][0], example_box[2][1]]\n",
    "        for i, yolo_res in enumerate(results_yolo):\n",
    "            list_yolo = [yolo_res['xmin'], yolo_res['ymin'],\n",
    "                        yolo_res['xmax'], yolo_res['ymax']]\n",
    "                        \n",
    "            if intersects(list_yolo, list_original) <= 0.10:\n",
    "                overlap.append(i+1)\n",
    "\n",
    "    non_overlap = list(set(non_overlap).difference(set(overlap)))\n",
    "    results_yolo = [results_yolo[i-1] for i in non_overlap]\n",
    "\n",
    "    # Keep only the most probable boxes\n",
    "    list_ = []\n",
    "    for i in results_yolo:\n",
    "        list_.append(i['confidence'])\n",
    "    order = list(np.argsort(list_)[::-1])\n",
    "    if isinstance(threshold, int):\n",
    "        results_yolo = [results_yolo[i] for i in order[:threshold]]\n",
    "    elif isinstance(threshold, float):\n",
    "        order_float = list(np.sort(list_)[::-1])\n",
    "        pos = len([i for i in order_float if i >= threshold])\n",
    "        results_yolo = [results_yolo[i] for i in order[:pos]]\n",
    "\n",
    "    # Return boxes as in the annotation file\n",
    "    final_list = []\n",
    "    for i in results_yolo:\n",
    "        final_list.append([i['xmin'], i['ymin'], i['xmax'], i['ymax']])\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Using CPU mode.\n"
     ]
    }
   ],
   "source": [
    "if not exists(anno_file) or not exists(im_dir):\n",
    "    print(\"Make sure you set up the --data-path correctly.\")\n",
    "    print(\"Current setting is {}, but the image dir and annotation file do not exist.\".format(data_path))\n",
    "    print(\"Aborting the evaluation\")\n",
    "    exit(-1)\n",
    "\n",
    "if not torch.cuda.is_available() or gpu_id < 0:\n",
    "    use_gpu = False\n",
    "    print(\"===> Using CPU mode.\")\n",
    "else:\n",
    "    use_gpu = True\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(anno_file) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "with open(data_split_file) as f:\n",
    "    data_split = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, num_img, pre_trained_backbone, yolo_flag, yolo_threshold, plot_flag=False, im_dir=im_dir, use_gpu=False, annotations=annotations, model_path='model.pth',\n",
    "         adapt=True, gradient_steps=100, learning_rate=1e-7):\n",
    "\n",
    "    weight_mincount = 1e-9\n",
    "    weight_perturbation = 1e-4\n",
    "\n",
    "    if pre_trained_backbone == 'vgg16':\n",
    "        backbone = VGG16FPN()\n",
    "    elif pre_trained_backbone == 'resnet':\n",
    "        backbone = Resnet50FPN()\n",
    "    elif pre_trained_backbone == 'wide_resnet':\n",
    "        backbone = Wide_Resnet50_2()\n",
    "\n",
    "    if use_gpu: backbone.cuda()\n",
    "    backbone.eval()\n",
    "\n",
    "    regressor = CountRegressor(6, pool='mean')\n",
    "    \n",
    "    if use_gpu:\n",
    "        regressor.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        regressor.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
    "\n",
    "    if use_gpu: regressor.cuda()\n",
    "    regressor.eval()\n",
    "\n",
    "    cnt = 0\n",
    "    SAE = 0  # sum of absolute errors\n",
    "    SSE = 0  # sum of square errors\n",
    "\n",
    "    print(\"Evaluation on {} data\".format(test_split))\n",
    "    n_imgs = num_img\n",
    "    im_ids = data[:n_imgs]\n",
    "\n",
    "    pbar = tqdm(im_ids)\n",
    "    for im_id in pbar:\n",
    "        anno = annotations[im_id]\n",
    "        bboxes = anno['box_examples_coordinates']\n",
    "        dots = np.array(anno['points'])\n",
    "        image_path = '{}/{}'.format(im_dir, im_id)\n",
    "\n",
    "        rects = list()\n",
    "        for bbox in bboxes:\n",
    "            x1, y1 = bbox[0][0], bbox[0][1]\n",
    "            x2, y2 = bbox[2][0], bbox[2][1]\n",
    "            rects.append([y1, x1, y2, x2])\n",
    "\n",
    "        if yolo_flag:\n",
    "                \n",
    "            detections = model_yolo(image_path)\n",
    "            results_yolo = detections.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "\n",
    "            try:\n",
    "                yolo_obj_cnt = count_class(results_yolo)\n",
    "            except:\n",
    "                print(\"Yolo Failed\")\n",
    "                yolo_obj_cnt=0\n",
    "\n",
    "            for result in results_yolo:\n",
    "                con = result['confidence']\n",
    "                cs = result['class']\n",
    "                x1 = int(result['xmin'])\n",
    "                y1 = int(result['ymin'])\n",
    "                x2 = int(result['xmax'])\n",
    "                y2 = int(result['ymax'])\n",
    "\n",
    "            yolo_res = YOLO_boxes(results_yolo, bboxes, threshold=yolo_threshold)\n",
    "\n",
    "            frame_1 = cv2.imread(image_path)\n",
    "            frame_1 = cv2.cvtColor(frame_1, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            frame_2 = cv2.imread(image_path)\n",
    "            frame_2 = cv2.cvtColor(frame_2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            for i in bboxes:\n",
    "                x1 = i[0][0]\n",
    "                y1 = i[0][1]\n",
    "                x2 = i[2][0]\n",
    "                y2 = i[2][1]\n",
    "                # Do whatever you want\n",
    "                f1 = cv2.rectangle(frame_1, (x1, y1), (x2, y2),color=(255,0,0),thickness=3)\n",
    "            \n",
    "            if plot_flag:\n",
    "                plt.imshow(f1)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            print('')\n",
    "\n",
    "            if yolo_res:\n",
    "                for i in yolo_res:\n",
    "                    x1 = int(i[0])\n",
    "                    y1 = int(i[1])\n",
    "                    x2 = int(i[2])\n",
    "                    y2 = int(i[3])\n",
    "                    # Do whatever you want\n",
    "                    f2 = cv2.rectangle(frame_2, (x1, y1), (x2, y2),color=(0,255,0),thickness=3)\n",
    "            \n",
    "                if plot_flag:\n",
    "                    plt.imshow(f2)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "\n",
    "            if yolo_res:\n",
    "                rects += yolo_res\n",
    "\n",
    "        image = Image.open('{}/{}'.format(im_dir, im_id))\n",
    "        image_path = '{}/{}'.format(im_dir, im_id)\n",
    "        image.load()\n",
    "        image.show()\n",
    "        sample = {'image': image, 'lines_boxes': rects}\n",
    "        sample = Transform(sample)\n",
    "        image, boxes = sample['image'], sample['boxes']\n",
    "\n",
    "        if use_gpu:\n",
    "            image = image.cuda()\n",
    "            boxes = boxes.cuda()\n",
    "\n",
    "        with torch.no_grad(): features = extract_features(backbone, image.unsqueeze(0), boxes.unsqueeze(0), MAPS, Scales)\n",
    "\n",
    "        if not adapt:\n",
    "            with torch.no_grad(): output = regressor(features)\n",
    "        else:\n",
    "            features.required_grad = True\n",
    "            adapted_regressor = copy.deepcopy(regressor)\n",
    "            adapted_regressor.train()\n",
    "            optimizer = optim.Adam(adapted_regressor.parameters(), lr=learning_rate)\n",
    "            for step in range(0, gradient_steps):\n",
    "                optimizer.zero_grad()\n",
    "                output = adapted_regressor(features)\n",
    "                lCount = weight_mincount * MincountLoss(output, boxes)\n",
    "                lPerturbation = weight_perturbation * PerturbationLoss(output, boxes, sigma=8)\n",
    "                Loss = lCount + lPerturbation\n",
    "                # loss can become zero in some cases, where loss is a 0 valued scalar and not a tensor\n",
    "                # So Perform gradient descent only for non zero cases\n",
    "                if torch.is_tensor(Loss):\n",
    "                    Loss.backward()\n",
    "                    optimizer.step() \n",
    "            features.required_grad = False\n",
    "            output = adapted_regressor(features)\n",
    "\n",
    "\n",
    "        gt_cnt = dots.shape[0]\n",
    "        pred_cnt = output.sum().item()\n",
    "        cnt = cnt + 1\n",
    "        err = abs(gt_cnt - pred_cnt)\n",
    "        SAE += err\n",
    "        SSE += err**2\n",
    "\n",
    "        pbar.set_description('{:<8}: actual-predicted: {:6d}, {:6.1f}, error: {:6.1f}. Current MAE: {:5.2f}, RMSE: {:5.2f}, YOLO: {:6.1f}'.\\\n",
    "                            format(im_id, gt_cnt, pred_cnt, abs(pred_cnt - gt_cnt), SAE/cnt, (SSE/cnt)**0.5, yolo_obj_cnt))\n",
    "        print(\"\")\n",
    "\n",
    "    print('On {} data, MAE: {:6.2f}, RMSE: {:6.2f}'.format(test_split, SAE/cnt, (SSE/cnt)**0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(data=data_split['test'], num_img=10, pre_trained_backbone='vgg16', yolo_flag=True, yolo_threshold=3, plot_flag=True, im_dir=im_dir, use_gpu=False, annotations=annotations, model_path=model_path,\n",
    "         adapt=False, gradient_steps=100, learning_rate=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alessandroquattrociocchi/Documents/Courses /2/2.1/AML/homeworks/FinalProject/Final_Project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6576bbb3ea8043231b63aded6960e6ea5b863559976330c0515cb191a60ecccc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('fscount': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
